{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1818f2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702bb01a",
   "metadata": {},
   "source": [
    "# Copied code from keen-source\n",
    "\n",
    "Find a cleaner fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0438efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Deterministic Run\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "torch.set_num_threads(1)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"adam\": optim.AdamW,\n",
    "    \"sgd\": optim.SGD\n",
    "}\n",
    "\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, optimizer, learning_rate, max_iter, T_max=None):\n",
    "        super(MLPRegressor, self).__init__()\n",
    "        self.max_iter = max_iter\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.last_activation = torch.sigmoid\n",
    "        self.layers =  nn.ModuleList(\n",
    "            [nn.Linear(input_size, 1, bias=False)]\n",
    "        )\n",
    "        self.best_weights = []\n",
    "        self.init_weights = []\n",
    "        self._initialize_weights()\n",
    "        self.optimizer = optimizers[optimizer](self.parameters(), lr=learning_rate)\n",
    "        self.best_train_loss, self.best_test_pearson_corr = 1, -1\n",
    "        self.initial_train, self.initial_test, self.final_train, self.final_test = None, None, None, None \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            torch.nn.init.kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "            self.init_weights.append(layer.weight)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            self.best_weights.append(layer.weight)\n",
    "\n",
    "    def set_to_best_weights(self):\n",
    "        for i, weight in enumerate(self.best_weights):\n",
    "            self.layers[i].weight = weight\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.layers[0](x)\n",
    "        return self.last_activation(x) \n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "    \n",
    "    def validate(self, X_test, y_test):\n",
    "            preds = self.predict(X_test)\n",
    "            test_loss = self.criterion(preds, y_test).item()\n",
    "\n",
    "            result_df = pd.DataFrame(\n",
    "                {\n",
    "                    \"preds\": preds.squeeze(dim=-1).detach().cpu().numpy(), \n",
    "                    \"target\": y_test.squeeze(dim=-1).detach().cpu().numpy()\n",
    "                }\n",
    "            )\n",
    "            test_pearson_corr, test_pearson_p_value = pearsonr(result_df[\"preds\"], result_df[\"target\"])\n",
    "            return result_df, test_loss, test_pearson_corr, test_pearson_p_value\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test):     \n",
    "        X_test = torch.tensor(X_test.tolist(), dtype=torch.float32).cuda()  \n",
    "        y_test = torch.tensor(y_test.tolist(), dtype=torch.float32).unsqueeze(dim=1).cuda()\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            print(f\"Epoch {epoch+1}/{self.max_iter}\")\n",
    "            epoch_train_loss = 0.0\n",
    "            n_examples = 0\n",
    "            for batch in X_train:\n",
    "                # Train\n",
    "                self.optimizer.zero_grad()\n",
    "                preds = self.forward(batch[0].to(torch.float32).cuda())\n",
    "                train_loss = self.criterion(preds, batch[1].to(torch.float32).unsqueeze(dim=1).cuda())\n",
    "                train_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += train_loss.item()\n",
    "                n_examples += batch[0].shape[0]\n",
    "            \n",
    "            # Test \n",
    "            epoch_train_loss = epoch_train_loss / y_train.shape[0]\n",
    "            result_df, test_loss, test_pearson_corr, test_pearson_p_value = self.validate(X_test, y_test)\n",
    "            \n",
    "            if epoch == 0:\n",
    "                self.initial_test = result_df\n",
    "            if test_pearson_corr > self.best_test_pearson_corr:\n",
    "                self.best_test_pearson_corr = test_pearson_corr\n",
    "                self.best_weights = [copy.deepcopy(l.weight)for l in self.layers]\n",
    "                self.final_test = result_df\n",
    "\n",
    "            print({\"test_loss\": test_loss, \"train_loss\": epoch_train_loss, \"test_pearson_corr\": test_pearson_corr})     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac7c53",
   "metadata": {},
   "source": [
    "# Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7717f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a63e14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NPZ_PATH = 'data/filtered_vectors.npz' \n",
    "\n",
    "try:\n",
    "    with np.load(INPUT_NPZ_PATH) as data:\n",
    "        norm_reps = data['representations_normalized']\n",
    "        row_indices = data['row_indices']\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{INPUT_NPZ_PATH}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf4e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open('data/score_by_entity.json', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a dictionary mapping vector_index to score for O(1) lookup\n",
    "score_dict = {item['vector_index']: item['score'] for item in data}\n",
    "\n",
    "# Create scores list in the order determined by row_indices\n",
    "scores = np.array([score_dict[idx] for idx in row_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728d428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = norm_reps, scores\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "\n",
    "# Convert to PyTorch tensors and create DataLoader\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "# Create DataLoader with batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=hyperparams.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c2765",
   "metadata": {},
   "source": [
    "# Probe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600310d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(\n",
       "  (criterion): MSELoss()\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=3072, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate MLPRegressor with the hyperparameters\n",
    "model = MLPRegressor(\n",
    "    input_size=X.shape[1],\n",
    "    optimizer=hyperparams.OPTIM,\n",
    "    learning_rate=hyperparams.LR,\n",
    "    max_iter=hyperparams.MAX_ITER\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59223053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sofia\\AppData\\Local\\Temp\\ipykernel_23224\\1979267127.py:85: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  test_pearson_corr, test_pearson_p_value = pearsonr(result_df[\"preds\"], result_df[\"target\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011479860366662672, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 2/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073691190873452, 'test_pearson_corr': np.float32(0.0011790118)}\n",
      "Epoch 3/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105576884605257, 'test_pearson_corr': np.float32(0.008325754)}\n",
      "Epoch 4/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049671735838, 'test_pearson_corr': np.float32(0.007995705)}\n",
      "Epoch 5/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104767539610617, 'test_pearson_corr': np.float32(0.008222784)}\n",
      "Epoch 6/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105273840775444, 'test_pearson_corr': np.float32(0.008445206)}\n",
      "Epoch 7/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105280389907182, 'test_pearson_corr': np.float32(0.0086628925)}\n",
      "Epoch 8/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109763798164672, 'test_pearson_corr': np.float32(0.008876391)}\n",
      "Epoch 9/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048349667447737, 'test_pearson_corr': np.float32(0.009085293)}\n",
      "Epoch 10/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071906881004393, 'test_pearson_corr': np.float32(0.009289771)}\n",
      "Epoch 11/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516044610265, 'test_pearson_corr': np.float32(0.009489857)}\n",
      "Epoch 12/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052460233906794, 'test_pearson_corr': np.float32(0.00968638)}\n",
      "Epoch 13/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011090360999651441, 'test_pearson_corr': np.float32(0.009877764)}\n",
      "Epoch 14/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049974961859527, 'test_pearson_corr': np.float32(0.010066013)}\n",
      "Epoch 15/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071772479428598, 'test_pearson_corr': np.float32(0.010249614)}\n",
      "Epoch 16/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108211368293659, 'test_pearson_corr': np.float32(0.010430292)}\n",
      "Epoch 17/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067358563007334, 'test_pearson_corr': np.float32(0.010606599)}\n",
      "Epoch 18/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057288272678643, 'test_pearson_corr': np.float32(0.010779039)}\n",
      "Epoch 19/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071335903104136, 'test_pearson_corr': np.float32(0.010948092)}\n",
      "Epoch 20/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108110719256314, 'test_pearson_corr': np.float32(0.011113885)}\n",
      "Epoch 21/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078273235482264, 'test_pearson_corr': np.float32(0.011276402)}\n",
      "Epoch 22/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050717878445494, 'test_pearson_corr': np.float32(0.011434109)}\n",
      "Epoch 23/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052879213474152, 'test_pearson_corr': np.float32(0.011589705)}\n",
      "Epoch 24/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055932001865867, 'test_pearson_corr': np.float32(0.01174171)}\n",
      "Epoch 25/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075300273128775, 'test_pearson_corr': np.float32(0.011890111)}\n",
      "Epoch 26/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049937143578852, 'test_pearson_corr': np.float32(0.012036016)}\n",
      "Epoch 27/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047777964771016, 'test_pearson_corr': np.float32(0.012177962)}\n",
      "Epoch 28/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059636236779148, 'test_pearson_corr': np.float32(0.012318069)}\n",
      "Epoch 29/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279142032775, 'test_pearson_corr': np.float32(0.0124550015)}\n",
      "Epoch 30/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056122468436074, 'test_pearson_corr': np.float32(0.012588571)}\n",
      "Epoch 31/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049595031413304, 'test_pearson_corr': np.float32(0.012719979)}\n",
      "Epoch 32/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057991722562065, 'test_pearson_corr': np.float32(0.012848202)}\n",
      "Epoch 33/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107071892900709, 'test_pearson_corr': np.float32(0.01297375)}\n",
      "Epoch 34/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105998389602201, 'test_pearson_corr': np.float32(0.013096769)}\n",
      "Epoch 35/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079621587754345, 'test_pearson_corr': np.float32(0.013216773)}\n",
      "Epoch 36/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011098015779619286, 'test_pearson_corr': np.float32(0.013335813)}\n",
      "Epoch 37/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011096058126186623, 'test_pearson_corr': np.float32(0.013450852)}\n",
      "Epoch 38/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052763587989514, 'test_pearson_corr': np.float32(0.013563998)}\n",
      "Epoch 39/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077130410687348, 'test_pearson_corr': np.float32(0.013674142)}\n",
      "Epoch 40/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011088854577145976, 'test_pearson_corr': np.float32(0.013783017)}\n",
      "Epoch 41/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895605644087, 'test_pearson_corr': np.float32(0.013889605)}\n",
      "Epoch 42/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104726131279202, 'test_pearson_corr': np.float32(0.013993715)}\n",
      "Epoch 43/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105551101539205, 'test_pearson_corr': np.float32(0.014095471)}\n",
      "Epoch 44/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068302898160453, 'test_pearson_corr': np.float32(0.014195465)}\n",
      "Epoch 45/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106702384119784, 'test_pearson_corr': np.float32(0.014293607)}\n",
      "Epoch 46/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080561476874467, 'test_pearson_corr': np.float32(0.014389438)}\n",
      "Epoch 47/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052670496828742, 'test_pearson_corr': np.float32(0.014483712)}\n",
      "Epoch 48/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052906610870257, 'test_pearson_corr': np.float32(0.014575286)}\n",
      "Epoch 49/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055229649665293, 'test_pearson_corr': np.float32(0.014664402)}\n",
      "Epoch 50/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047952667995317, 'test_pearson_corr': np.float32(0.014753614)}\n",
      "Epoch 51/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047819270740226, 'test_pearson_corr': np.float32(0.01483933)}\n",
      "Epoch 52/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058464749048613, 'test_pearson_corr': np.float32(0.014924888)}\n",
      "Epoch 53/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591120878215, 'test_pearson_corr': np.float32(0.015007982)}\n",
      "Epoch 54/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104967170764502, 'test_pearson_corr': np.float32(0.015088468)}\n",
      "Epoch 55/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051693874966123, 'test_pearson_corr': np.float32(0.01516843)}\n",
      "Epoch 56/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105620582271735, 'test_pearson_corr': np.float32(0.015246323)}\n",
      "Epoch 57/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050547188708813, 'test_pearson_corr': np.float32(0.0153232785)}\n",
      "Epoch 58/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045830245243404, 'test_pearson_corr': np.float32(0.015397631)}\n",
      "Epoch 59/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048458941921387, 'test_pearson_corr': np.float32(0.015471877)}\n",
      "Epoch 60/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048308531341235, 'test_pearson_corr': np.float32(0.015543783)}\n",
      "Epoch 61/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078098405172684, 'test_pearson_corr': np.float32(0.015614231)}\n",
      "Epoch 62/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105370649571035, 'test_pearson_corr': np.float32(0.015683118)}\n",
      "Epoch 63/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104893507278037, 'test_pearson_corr': np.float32(0.01575046)}\n",
      "Epoch 64/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052130425916902, 'test_pearson_corr': np.float32(0.015817443)}\n",
      "Epoch 65/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109242365545376, 'test_pearson_corr': np.float32(0.015883027)}\n",
      "Epoch 66/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067661026734046, 'test_pearson_corr': np.float32(0.015946735)}\n",
      "Epoch 67/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054523060276644, 'test_pearson_corr': np.float32(0.01600941)}\n",
      "Epoch 68/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053106551474632, 'test_pearson_corr': np.float32(0.016070379)}\n",
      "Epoch 69/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110693164054414, 'test_pearson_corr': np.float32(0.016130682)}\n",
      "Epoch 70/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107056250014733, 'test_pearson_corr': np.float32(0.016189788)}\n",
      "Epoch 71/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087641826711613, 'test_pearson_corr': np.float32(0.01624723)}\n",
      "Epoch 72/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279182587293, 'test_pearson_corr': np.float32(0.016303753)}\n",
      "Epoch 73/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075039069503463, 'test_pearson_corr': np.float32(0.01635985)}\n",
      "Epoch 74/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104885778519472, 'test_pearson_corr': np.float32(0.016414316)}\n",
      "Epoch 75/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086051528680038, 'test_pearson_corr': np.float32(0.016467847)}\n",
      "Epoch 76/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047758086117458, 'test_pearson_corr': np.float32(0.01652229)}\n",
      "Epoch 77/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068370764085818, 'test_pearson_corr': np.float32(0.01657668)}\n",
      "Epoch 78/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048055318202935, 'test_pearson_corr': np.float32(0.016630137)}\n",
      "Epoch 79/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056980949674903, 'test_pearson_corr': np.float32(0.016684484)}\n",
      "Epoch 80/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054320002612992, 'test_pearson_corr': np.float32(0.016742108)}\n",
      "Epoch 81/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052178889650076, 'test_pearson_corr': np.float32(0.016806494)}\n",
      "Epoch 82/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080025259400863, 'test_pearson_corr': np.float32(0.016880248)}\n",
      "Epoch 83/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051255666159666, 'test_pearson_corr': np.float32(0.016970059)}\n",
      "Epoch 84/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075410993902316, 'test_pearson_corr': np.float32(0.01708542)}\n",
      "Epoch 85/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071908739572404, 'test_pearson_corr': np.float32(0.017246686)}\n",
      "Epoch 86/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074634732611043, 'test_pearson_corr': np.float32(0.017475387)}\n",
      "Epoch 87/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055971783896057, 'test_pearson_corr': np.float32(0.017831197)}\n",
      "Epoch 88/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066949615586722, 'test_pearson_corr': np.float32(0.018375877)}\n",
      "Epoch 89/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105374678131054, 'test_pearson_corr': np.float32(0.01924343)}\n",
      "Epoch 90/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067723728572893, 'test_pearson_corr': np.float32(0.020772718)}\n",
      "Epoch 91/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104751604547774, 'test_pearson_corr': np.float32(0.0235521)}\n",
      "Epoch 92/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105159115199893, 'test_pearson_corr': np.float32(0.029726628)}\n",
      "Epoch 93/20972\n",
      "{'test_loss': 0.03678823262453079, 'train_loss': 0.0011086391503492372, 'test_pearson_corr': np.float32(0.0664946)}\n",
      "Epoch 94/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02509707695484958, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 95/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326628460436393, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 96/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832922499332627, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 97/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326266579851843, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 98/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329541171094524, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 99/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331437926800526, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 100/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833111671905759, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 101/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326942262013516, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 102/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832993420865248, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 103/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328304463545007, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 104/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832757039814521, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 105/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028322385737863987, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 106/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833003279837695, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 107/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832746469095105, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 108/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832902582631559, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 109/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330903150552075, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 110/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329903762315672, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 111/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327568543827834, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 112/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329616043322486, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 113/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833134575501284, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 114/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330237761517395, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 115/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832819239393254, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 116/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327421231081517, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 117/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328348978265742, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 118/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283244007923564, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 119/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330523759437568, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 120/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331785317060668, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 121/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326518589355906, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 122/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833001593186141, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 123/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325704877138494, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 124/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328163790809414, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 125/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832979936535774, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 126/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833024001556786, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 127/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283292684642995, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 128/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328890283487947, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 129/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329176259200724, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 130/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833068034377077, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 131/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329497444736264, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 132/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832867271765156, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 133/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325325519335073, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 134/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329874770563156, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 135/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327636465141564, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 136/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328711014302052, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 137/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832890432966448, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 138/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326550690143545, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 139/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329510769172502, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 140/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326841817971668, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 141/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832864852269609, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 142/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330496766350487, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 143/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832964774437704, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 144/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832940528405228, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 145/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832545597666955, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 146/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325160362947892, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 147/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329360336287367, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 148/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329206516774685, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 149/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329982476422755, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 150/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330346355672744, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 151/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832830441913022, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 152/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833049798775714, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 153/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832382343343047, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 154/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326131136957474, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 155/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833108668355757, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 156/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329251975309652, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 157/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832928932814591, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 158/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329534486669007, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 159/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832991845250663, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 160/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832963137752786, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 161/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832903936172207, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 162/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327085610740054, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 163/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832677878228456, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 164/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329982509733845, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 165/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327370476083143, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 166/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329056194926518, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 167/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329405239637492, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 168/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832901437840412, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 169/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283273136029478, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 170/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332893310556113, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 171/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331222315214784, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 172/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832981786411672, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 173/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833167969869608, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 174/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326652277866172, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 175/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833111673016129, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 176/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833003868333629, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 177/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326708951134972, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 178/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331496032446224, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 179/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328452697898167, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 180/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326022609424305, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 181/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329118020310663, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 182/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328880900864096, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 183/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330267819224812, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 184/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833025198535308, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 185/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283321947436752, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 186/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328297212830953, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 187/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329915365678898, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 188/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832814392629572, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 189/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329588939198437, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 190/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325841963379998, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 191/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832855400802837, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 192/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329287296169382, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 193/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327658994542503, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 194/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832817688206802, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 195/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833026764156566, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 196/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833026770818784, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 197/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331815263731113, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 198/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832951071365402, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 199/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330184152868927, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 200/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832797622716196, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 201/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330680488118827, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 202/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330599220161467, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 203/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324767969404353, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 204/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329497544669537, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 205/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327223518655124, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 206/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283268937943767, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 207/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326320510507517, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 208/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326072998000683, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 209/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832831366850971, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 210/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329903729004582, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 211/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330892557625265, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 212/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330844922765712, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 213/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832782862571952, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 214/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329981832408337, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 215/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028323718592324663, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 216/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326192873512047, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 217/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329601530790685, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 218/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326492517875664, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 219/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328540605866252, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 220/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833095280628446, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 221/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328128692023623, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 222/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325859040865778, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 223/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329359803309917, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 224/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328556228767742, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 225/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329120962790333, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 226/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327508639383246, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 227/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329972760687997, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 228/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283248152822567, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 229/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832836787675781, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 230/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328297457112284, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 231/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330146333677402, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 232/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329546400935747, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 233/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326114858937868, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 234/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833158950336644, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 235/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832734133998257, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 236/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332300328728695, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 237/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329864577369433, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 238/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324939221721114, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 239/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325737677459033, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 240/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832977041802001, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 241/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832561996716856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 242/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326471542992285, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 243/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327223385410764, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 244/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326913603371907, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 245/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833103984816419, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 246/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832371378442392, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 247/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330496799661577, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 248/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028323668836659006, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 249/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832724884618768, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 250/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326566013245222, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 251/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326646070899623, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 252/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331116774576077, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 253/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328095880599383, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 254/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329042659520038, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 255/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331345710598053, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 256/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329283953956626, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 257/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327949811467114, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 258/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832861850940346, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 259/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328073773138923, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 260/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832614407276432, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 261/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324521967000294, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 262/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325599836166143, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 263/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328419497844537, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 264/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028322515506769258, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 265/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329050132308027, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 266/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327008806468833, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 267/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327355241811045, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 268/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328406328860054, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 269/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832978891677899, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 270/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832764834609721, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 271/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327219321457712, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 272/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832855630649362, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 273/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328378902728796, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 274/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329970773126257, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 275/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832973903897267, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 276/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832383030661883, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 277/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328527581229827, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 278/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327615623502545, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 279/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832653058134852, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 280/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832577715110139, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 281/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832958886147256, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 282/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832403510320382, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 283/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330495844743647, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 284/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832917368314305, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 285/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329195923847875, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 286/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331195355438796, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 287/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832998256525233, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 288/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.01618186272741049, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 289/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011081911678389187, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 290/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516026067958, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 291/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052930272045863, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 292/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052063444817381, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 293/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108891858453668, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 294/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071638162865479, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 295/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070012385594684, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 296/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011104584815167654, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 297/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053015173471154, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 298/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105636803102964, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 299/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072209081126742, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 300/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067391126767085, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 301/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072175433021593, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 302/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072717874637775, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 303/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108005850000901, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 304/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053543977709015, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 305/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048458907222333, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 306/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059810315854238, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 307/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107017968100497, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 308/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059373567118857, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 309/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073793047557108, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 310/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066937519605383, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 311/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051624384474483, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 312/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075782011788417, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 313/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049496264897158, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 314/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105644248955815, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 315/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053746783479233, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 316/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108464046524374, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 317/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054919359784112, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 318/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105583603664762, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 319/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110894918000921, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 320/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065096706124203, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 321/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077728167364661, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 322/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047381786601798, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 323/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080698824182968, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 324/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052534006044668, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 325/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105245294623825, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 326/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059799390857493, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 327/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072310254564364, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 328/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051290404899413, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 329/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062786510858224, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 330/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057121663008388, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 331/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051987440012334, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 332/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052730464165366, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 333/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046226546594258, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 334/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104868854989052, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 335/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056851742764166, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 336/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072605333465183, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 337/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106966937179594, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 338/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071838735101048, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 339/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110674604473418, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 340/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071462184667862, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 341/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049561799263052, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 342/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110561725306028, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 343/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067460457968384, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 344/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068125190431927, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 345/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110688399345317, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 346/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105253407652712, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 347/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047682724979841, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 348/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059440561122088, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 349/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048795092464376, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 350/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057385365616102, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 351/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106655083759098, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 352/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071335864826744, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 353/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069743082776733, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 354/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055560363734198, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 355/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074064257304878, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 356/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058064446029504, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 357/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108372596032406, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 358/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075039101600087, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 359/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591177264177, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 360/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105570483063771, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 361/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105169228076152, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 362/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057896958473214, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 363/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053302856919498, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 364/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048055281985799, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 365/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072310255323405, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 366/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051962304452292, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 367/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059567526148719, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 368/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045527109656283, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 369/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055811726274427, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 370/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047618686576858, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 371/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047385277977108, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 372/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049370823484248, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 373/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895557065413, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 374/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048221924837022, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 375/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107679040692299, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 376/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105223301019632, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 377/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053746830051866, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 378/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056678728516257, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 379/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072414444217142, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 380/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728860647524, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 381/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066890778138, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 382/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087698461312659, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 383/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052922929509442, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 384/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051693746362758, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 385/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054514255825758, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 386/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051077813345727, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 387/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065328438976576, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 388/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049838432039018, 'test_pearson_corr': np.float32(-0.014431554)}\n",
      "Epoch 389/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056005769774795, 'test_pearson_corr': np.float32(0.019046182)}\n",
      "Epoch 390/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078907801348428, 'test_pearson_corr': np.float32(0.011265186)}\n",
      "Epoch 391/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054894229212057, 'test_pearson_corr': np.float32(0.011321535)}\n",
      "Epoch 392/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061986617343369, 'test_pearson_corr': np.float32(0.0113770515)}\n",
      "Epoch 393/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061387528222431, 'test_pearson_corr': np.float32(0.011431391)}\n",
      "Epoch 394/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050143029865908, 'test_pearson_corr': np.float32(0.011484371)}\n",
      "Epoch 395/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065538530029779, 'test_pearson_corr': np.float32(0.011534661)}\n",
      "Epoch 396/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050108414524668, 'test_pearson_corr': np.float32(0.011584257)}\n",
      "Epoch 397/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011096323216864881, 'test_pearson_corr': np.float32(0.011632431)}\n",
      "Epoch 398/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072577957213813, 'test_pearson_corr': np.float32(0.011679516)}\n",
      "Epoch 399/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065708380157818, 'test_pearson_corr': np.float32(0.011726024)}\n",
      "Epoch 400/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046303220006676, 'test_pearson_corr': np.float32(0.011770042)}\n",
      "Epoch 401/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106493220637322, 'test_pearson_corr': np.float32(0.011814654)}\n",
      "Epoch 402/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067190181625133, 'test_pearson_corr': np.float32(0.011856365)}\n",
      "Epoch 403/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011091533536377715, 'test_pearson_corr': np.float32(0.011896308)}\n",
      "Epoch 404/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050917611831463, 'test_pearson_corr': np.float32(0.011935723)}\n",
      "Epoch 405/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105728488127998, 'test_pearson_corr': np.float32(0.011975726)}\n",
      "Epoch 406/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106938543562842, 'test_pearson_corr': np.float32(0.012012348)}\n",
      "Epoch 407/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105186040656351, 'test_pearson_corr': np.float32(0.012050603)}\n",
      "Epoch 408/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052452874671455, 'test_pearson_corr': np.float32(0.012085125)}\n",
      "Epoch 409/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105843398139858, 'test_pearson_corr': np.float32(0.012120253)}\n",
      "Epoch 410/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069076874844408, 'test_pearson_corr': np.float32(0.012153113)}\n",
      "Epoch 411/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107272794430287, 'test_pearson_corr': np.float32(0.012186562)}\n",
      "Epoch 412/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011105553252824954, 'test_pearson_corr': np.float32(0.012218823)}\n",
      "Epoch 413/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055140648763794, 'test_pearson_corr': np.float32(0.012249012)}\n",
      "Epoch 414/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357896568653, 'test_pearson_corr': np.float32(0.012279683)}\n",
      "Epoch 415/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052897704165602, 'test_pearson_corr': np.float32(0.012309652)}\n",
      "Epoch 416/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105077462289073, 'test_pearson_corr': np.float32(0.012337797)}\n",
      "Epoch 417/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357824784988, 'test_pearson_corr': np.float32(0.012365167)}\n",
      "Epoch 418/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072311751286314, 'test_pearson_corr': np.float32(0.012393245)}\n",
      "Epoch 419/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054987080515628, 'test_pearson_corr': np.float32(0.012419652)}\n",
      "Epoch 420/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069666331725183, 'test_pearson_corr': np.float32(0.012445088)}\n",
      "Epoch 421/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076224305974591, 'test_pearson_corr': np.float32(0.012470364)}\n",
      "Epoch 422/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048255898679527, 'test_pearson_corr': np.float32(0.012493745)}\n",
      "Epoch 423/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050077656200002, 'test_pearson_corr': np.float32(0.012517845)}\n",
      "Epoch 424/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068333143156004, 'test_pearson_corr': np.float32(0.012540357)}\n",
      "Epoch 425/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051051263040246, 'test_pearson_corr': np.float32(0.012563508)}\n",
      "Epoch 426/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057454972783307, 'test_pearson_corr': np.float32(0.012585748)}\n",
      "Epoch 427/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055458990017982, 'test_pearson_corr': np.float32(0.012607024)}\n",
      "Epoch 428/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065034760507836, 'test_pearson_corr': np.float32(0.012627356)}\n",
      "Epoch 429/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048225422501537, 'test_pearson_corr': np.float32(0.01264712)}\n",
      "Epoch 430/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011064932222204663, 'test_pearson_corr': np.float32(0.012666791)}\n",
      "Epoch 431/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066684224761657, 'test_pearson_corr': np.float32(0.012686024)}\n",
      "Epoch 432/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051495263010164, 'test_pearson_corr': np.float32(0.012704956)}\n",
      "Epoch 433/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053342494406445, 'test_pearson_corr': np.float32(0.012723218)}\n",
      "Epoch 434/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069783399172332, 'test_pearson_corr': np.float32(0.012741755)}\n",
      "Epoch 435/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079600769298634, 'test_pearson_corr': np.float32(0.012758846)}\n",
      "Epoch 436/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049291773078285, 'test_pearson_corr': np.float32(0.012775123)}\n",
      "Epoch 437/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011097670270633349, 'test_pearson_corr': np.float32(0.012790739)}\n",
      "Epoch 438/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051010876812802, 'test_pearson_corr': np.float32(0.0128074465)}\n",
      "Epoch 439/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056020394232728, 'test_pearson_corr': np.float32(0.012822384)}\n",
      "Epoch 440/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054119436558192, 'test_pearson_corr': np.float32(0.0128377965)}\n",
      "Epoch 441/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066448189118313, 'test_pearson_corr': np.float32(0.012853481)}\n",
      "Epoch 442/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279149189455, 'test_pearson_corr': np.float32(0.012867341)}\n",
      "Epoch 443/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051962260536304, 'test_pearson_corr': np.float32(0.012881177)}\n",
      "Epoch 444/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069243577768368, 'test_pearson_corr': np.float32(0.012895114)}\n",
      "Epoch 445/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047618745998987, 'test_pearson_corr': np.float32(0.01290753)}\n",
      "Epoch 446/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058797294576895, 'test_pearson_corr': np.float32(0.012920802)}\n",
      "Epoch 447/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078478430065393, 'test_pearson_corr': np.float32(0.012933205)}\n",
      "Epoch 448/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109161987152449, 'test_pearson_corr': np.float32(0.012945954)}\n",
      "Epoch 449/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011105898126818228, 'test_pearson_corr': np.float32(0.012958248)}\n",
      "Epoch 450/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110597784365333, 'test_pearson_corr': np.float32(0.012970347)}\n",
      "Epoch 451/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058598190978733, 'test_pearson_corr': np.float32(0.01298263)}\n",
      "Epoch 452/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104839489224121, 'test_pearson_corr': np.float32(0.01299377)}\n",
      "Epoch 453/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058330091350433, 'test_pearson_corr': np.float32(0.013007199)}\n",
      "Epoch 454/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048292275214562, 'test_pearson_corr': np.float32(0.013020752)}\n",
      "Epoch 455/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046303275416725, 'test_pearson_corr': np.float32(0.013035348)}\n",
      "Epoch 456/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057321274297064, 'test_pearson_corr': np.float32(0.013053719)}\n",
      "Epoch 457/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076035078269, 'test_pearson_corr': np.float32(0.013077233)}\n",
      "Epoch 458/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050211001623384, 'test_pearson_corr': np.float32(0.013106831)}\n",
      "Epoch 459/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106921250042929, 'test_pearson_corr': np.float32(0.013148303)}\n",
      "Epoch 460/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059542931026423, 'test_pearson_corr': np.float32(0.013218445)}\n",
      "Epoch 461/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054042647988293, 'test_pearson_corr': np.float32(0.013319629)}\n",
      "Epoch 462/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047914941450263, 'test_pearson_corr': np.float32(0.013482717)}\n",
      "Epoch 463/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048294786775372, 'test_pearson_corr': np.float32(0.013757703)}\n",
      "Epoch 464/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104966439330158, 'test_pearson_corr': np.float32(0.014262618)}\n",
      "Epoch 465/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011097646640253153, 'test_pearson_corr': np.float32(0.015624125)}\n",
      "Epoch 466/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001279399099002624, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 467/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011064932190216474, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 468/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895577234236, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 469/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073523121782726, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 470/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591214674091, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 471/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050854177191997, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 472/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110774292855561, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 473/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050118288573852, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 474/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106532843930188, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 475/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056368128620727, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 476/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052930236696203, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 477/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066987431048098, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 478/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070752176988784, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 479/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011064495531807066, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 480/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068696959036848, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 481/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110858533142451, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 482/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048585704934468, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 483/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072105006956746, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 484/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046485105826451, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 485/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046462595249748, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 486/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110564424310035, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 487/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073111587656167, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 488/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052939827080632, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 489/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087462391295565, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 490/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049165463863752, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 491/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051447533161952, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 492/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057117475266464, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 493/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105196224589764, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 494/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045527012824238, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 495/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047043015955027, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 496/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066643919101077, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 497/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073354152251582, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 498/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076187055131862, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 499/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051930714000972, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 500/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087667956724819, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 501/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047136070248054, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 502/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105173260453067, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 503/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067627108301593, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 504/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108915812532652, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 505/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055971878450974, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 506/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061666641113191, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 507/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078041863716909, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 508/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055735957097635, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 509/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104713610266998, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 510/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048222062765758, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 511/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048476675739748, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 512/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110538657732539, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 513/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079828149480149, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 514/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049402282079168, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 515/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052264022257806, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 516/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065708284843858, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 517/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069432348530808, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 518/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728916599746, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 519/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080768430699565, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 520/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056005810654616, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 521/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053095981601005, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 522/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047079462105035, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 523/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104768278038989, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 524/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367341723404, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 525/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057939781441634, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 526/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050046928020143, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 527/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067864090036296, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 528/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045526996450623, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 529/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050240905483874, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 530/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105358022499008, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 531/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072888311179808, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 532/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105549184199674, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 533/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046842522876672, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 534/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049160934119289, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 535/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053546577752408, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 536/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105217027626085, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 537/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105411797160756, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 538/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078167358045876, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 539/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107126552409988, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 540/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057313895109768, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 541/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054148284916872, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 542/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728904238208, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 543/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054886714590023, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 544/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069036121674483, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 545/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050108381777436, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 546/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046066288694033, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 547/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077060374010933, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 548/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105913737587195, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 549/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105469188222838, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 550/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107018782921002, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 551/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367306807482, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 552/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011093492523446783, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 553/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046066273621633, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 554/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367334349856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 555/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066921166700884, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 556/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066104680315895, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 557/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011092786687678185, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 558/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054689655199802, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 559/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057319802731927, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 560/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011091614204084836, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 561/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046066269175818, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 562/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106586765314712, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 563/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011064137019242692, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 564/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106614501048268, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 565/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105182750383739, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 566/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056328334229, 'test_pearson_corr': np.float32(-0.01707284)}\n",
      "Epoch 567/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048357615916367, 'test_pearson_corr': np.float32(0.012210622)}\n",
      "Epoch 568/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011091876143763805, 'test_pearson_corr': np.float32(0.010542993)}\n",
      "Epoch 569/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047541942139816, 'test_pearson_corr': np.float32(0.010599126)}\n",
      "Epoch 570/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046605585274827, 'test_pearson_corr': np.float32(0.010654167)}\n",
      "Epoch 571/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048720028112092, 'test_pearson_corr': np.float32(0.01070776)}\n",
      "Epoch 572/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110502145146856, 'test_pearson_corr': np.float32(0.010760609)}\n",
      "Epoch 573/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059610675721971, 'test_pearson_corr': np.float32(0.010810646)}\n",
      "Epoch 574/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045527010655547, 'test_pearson_corr': np.float32(0.010859717)}\n",
      "Epoch 575/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054049973175189, 'test_pearson_corr': np.float32(0.010908538)}\n",
      "Epoch 576/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050917547312912, 'test_pearson_corr': np.float32(0.010955224)}\n",
      "Epoch 577/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105651251387303, 'test_pearson_corr': np.float32(0.011001529)}\n",
      "Epoch 578/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011085113462778267, 'test_pearson_corr': np.float32(0.011045753)}\n",
      "Epoch 579/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047507222809853, 'test_pearson_corr': np.float32(0.011089977)}\n",
      "Epoch 580/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107294486065974, 'test_pearson_corr': np.float32(0.011131922)}\n",
      "Epoch 581/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049300478962174, 'test_pearson_corr': np.float32(0.011173063)}\n",
      "Epoch 582/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106547139018209, 'test_pearson_corr': np.float32(0.011213377)}\n",
      "Epoch 583/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067460439426078, 'test_pearson_corr': np.float32(0.011253257)}\n",
      "Epoch 584/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895545354481, 'test_pearson_corr': np.float32(0.011291494)}\n",
      "Epoch 585/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051077779622586, 'test_pearson_corr': np.float32(0.011327795)}\n",
      "Epoch 586/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052738453730686, 'test_pearson_corr': np.float32(0.0113653755)}\n",
      "Epoch 587/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106052308124053, 'test_pearson_corr': np.float32(0.011400146)}\n",
      "Epoch 588/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047952702694368, 'test_pearson_corr': np.float32(0.011435171)}\n",
      "Epoch 589/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367312337644, 'test_pearson_corr': np.float32(0.011468973)}\n",
      "Epoch 590/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069513494542684, 'test_pearson_corr': np.float32(0.011501724)}\n",
      "Epoch 591/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050774568348155, 'test_pearson_corr': np.float32(0.011533651)}\n",
      "Epoch 592/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058598111387782, 'test_pearson_corr': np.float32(0.011565077)}\n",
      "Epoch 593/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728918931089, 'test_pearson_corr': np.float32(0.011595728)}\n",
      "Epoch 594/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110479526575856, 'test_pearson_corr': np.float32(0.011624801)}\n",
      "Epoch 595/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011063854916377935, 'test_pearson_corr': np.float32(0.011653698)}\n",
      "Epoch 596/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055560355710044, 'test_pearson_corr': np.float32(0.01168292)}\n",
      "Epoch 597/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047079442261514, 'test_pearson_corr': np.float32(0.011710237)}\n",
      "Epoch 598/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067115217466366, 'test_pearson_corr': np.float32(0.011737573)}\n",
      "Epoch 599/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357782495518, 'test_pearson_corr': np.float32(0.011763374)}\n",
      "Epoch 600/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069379213112357, 'test_pearson_corr': np.float32(0.011789295)}\n",
      "Epoch 601/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106493218013206, 'test_pearson_corr': np.float32(0.011814523)}\n",
      "Epoch 602/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071535509838562, 'test_pearson_corr': np.float32(0.011839877)}\n",
      "Epoch 603/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070052725195277, 'test_pearson_corr': np.float32(0.011861906)}\n",
      "Epoch 604/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105159113128793, 'test_pearson_corr': np.float32(0.011885756)}\n",
      "Epoch 605/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055467910276956, 'test_pearson_corr': np.float32(0.01190836)}\n",
      "Epoch 606/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048221902716377, 'test_pearson_corr': np.float32(0.011930676)}\n",
      "Epoch 607/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110612937185744, 'test_pearson_corr': np.float32(0.011952558)}\n",
      "Epoch 608/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050884538863115, 'test_pearson_corr': np.float32(0.011973809)}\n",
      "Epoch 609/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104995083940339, 'test_pearson_corr': np.float32(0.011993847)}\n",
      "Epoch 610/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105505345470703, 'test_pearson_corr': np.float32(0.012013342)}\n",
      "Epoch 611/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060556128618326, 'test_pearson_corr': np.float32(0.012033767)}\n",
      "Epoch 612/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054016766832355, 'test_pearson_corr': np.float32(0.012052852)}\n",
      "Epoch 613/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050378327829079, 'test_pearson_corr': np.float32(0.012070802)}\n",
      "Epoch 614/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050484441219026, 'test_pearson_corr': np.float32(0.01208946)}\n",
      "Epoch 615/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105694799077969, 'test_pearson_corr': np.float32(0.012107127)}\n",
      "Epoch 616/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058582541705962, 'test_pearson_corr': np.float32(0.012125123)}\n",
      "Epoch 617/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074343512678623, 'test_pearson_corr': np.float32(0.012142709)}\n",
      "Epoch 618/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076252483231905, 'test_pearson_corr': np.float32(0.01215835)}\n",
      "Epoch 619/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056022130378144, 'test_pearson_corr': np.float32(0.012175303)}\n",
      "Epoch 620/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048200314809171, 'test_pearson_corr': np.float32(0.012190109)}\n",
      "Epoch 621/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049830980634319, 'test_pearson_corr': np.float32(0.012206412)}\n",
      "Epoch 622/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728851322154, 'test_pearson_corr': np.float32(0.012222141)}\n",
      "Epoch 623/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052124115026691, 'test_pearson_corr': np.float32(0.012235899)}\n",
      "Epoch 624/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051692350810233, 'test_pearson_corr': np.float32(0.012250971)}\n",
      "Epoch 625/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050027251597002, 'test_pearson_corr': np.float32(0.012263926)}\n",
      "Epoch 626/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104738269289768, 'test_pearson_corr': np.float32(0.012277823)}\n",
      "Epoch 627/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104728256162442, 'test_pearson_corr': np.float32(0.012292599)}\n",
      "Epoch 628/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106767817229503, 'test_pearson_corr': np.float32(0.012306223)}\n",
      "Epoch 629/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068839891157883, 'test_pearson_corr': np.float32(0.012319593)}\n",
      "Epoch 630/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105159117379427, 'test_pearson_corr': np.float32(0.012332836)}\n",
      "Epoch 631/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051154521348762, 'test_pearson_corr': np.float32(0.012346786)}\n",
      "Epoch 632/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109296271293639, 'test_pearson_corr': np.float32(0.0123616)}\n",
      "Epoch 633/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077725879179005, 'test_pearson_corr': np.float32(0.01237691)}\n",
      "Epoch 634/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107571408492284, 'test_pearson_corr': np.float32(0.012396)}\n",
      "Epoch 635/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071369102507158, 'test_pearson_corr': np.float32(0.01241703)}\n",
      "Epoch 636/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048491937250605, 'test_pearson_corr': np.float32(0.012448403)}\n",
      "Epoch 637/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061293696453753, 'test_pearson_corr': np.float32(0.012485525)}\n",
      "Epoch 638/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050378287166126, 'test_pearson_corr': np.float32(0.012542134)}\n",
      "Epoch 639/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050647625008435, 'test_pearson_corr': np.float32(0.012643862)}\n",
      "Epoch 640/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048998154790713, 'test_pearson_corr': np.float32(0.0128036365)}\n",
      "Epoch 641/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067087807274984, 'test_pearson_corr': np.float32(0.013070198)}\n",
      "Epoch 642/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086891491685045, 'test_pearson_corr': np.float32(0.013531153)}\n",
      "Epoch 643/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045923406669757, 'test_pearson_corr': np.float32(0.014815931)}\n",
      "Epoch 644/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0012520108945244762, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 645/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108056449135467, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 646/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046066289778379, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 647/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053882735668947, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 648/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071265562051969, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 649/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052930253178253, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 650/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076941283283707, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 651/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105236733998845, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 652/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049031125071552, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 653/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074030957057732, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 654/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053706475758396, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 655/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060857084345893, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 656/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069441933818814, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 657/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591146468767, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 658/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053073227588759, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 659/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047682709582137, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 660/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068673327247003, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 661/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087302142070105, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 662/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104768278396823, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 663/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066144907253, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 664/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053032840710709, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 665/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110521303838443, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 666/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105289978757263, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 667/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072638619832706, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 668/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074871050401058, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 669/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048594567614701, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 670/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049671815428952, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 671/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052130329193292, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 672/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049251446489836, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 673/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055262884417973, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 674/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056981655800627, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 675/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051894288778828, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 676/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069076894796362, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 677/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069987212516293, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 678/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591181601558, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 679/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107473477681137, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 680/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105732031660321, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 681/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072065430084708, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 682/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105267058108238, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 683/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057917145948341, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 684/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067728886044906, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 685/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066487127527782, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 686/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048855218982903, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 687/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053343228508278, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 688/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065471403085801, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 689/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053681291944982, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 690/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109454605500068, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 691/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048728849912504, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 692/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895523016967, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 693/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071392573163278, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 694/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071839989254934, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 695/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104896840946152, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 696/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052533990213225, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 697/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051051256100435, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 698/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074333751401357, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 699/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050784140298714, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 700/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053246193691733, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 701/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052097406948954, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 702/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050875669351506, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 703/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061159311793745, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 704/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050614609727262, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 705/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011109740989001318, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 706/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070989175422405, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 707/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516058056148, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 708/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055255527351324, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 709/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086326263743125, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 710/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106921254510432, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 711/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104963927097055, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 712/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075680190454418, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 713/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049031240337468, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 714/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060726087614645, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 715/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065471427266704, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 716/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050244035338429, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 717/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047885556449044, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 718/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053327248076424, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 719/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048762121424495, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 720/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011085416739438723, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 721/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739906825933, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 722/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058868102548191, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 723/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066541239399263, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 724/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056106325569278, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 725/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049013431482407, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 726/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048895537113456, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 727/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051843779428061, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 728/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050282470828501, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 729/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066263923702564, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 730/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067864080060317, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 731/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055569160377799, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 732/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104832378390624, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 733/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105317505499509, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 734/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050378331298983, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 735/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047136195923685, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 736/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051651925492137, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 737/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053483117088652, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 738/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105334318578507, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 739/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057521788845458, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 740/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072311759744208, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 741/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070688535239348, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 742/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011085272728394017, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 743/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105668069703689, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 744/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050068048899784, 'test_pearson_corr': np.float32(-0.0010414637)}\n",
      "Epoch 745/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049434811031432, 'test_pearson_corr': np.float32(0.013086435)}\n",
      "Epoch 746/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049371676864076, 'test_pearson_corr': np.float32(0.008653395)}\n",
      "Epoch 747/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049944255258138, 'test_pearson_corr': np.float32(0.00869564)}\n",
      "Epoch 748/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055768149035349, 'test_pearson_corr': np.float32(0.008737628)}\n",
      "Epoch 749/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107024930573857, 'test_pearson_corr': np.float32(0.008777266)}\n",
      "Epoch 750/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105489415688622, 'test_pearson_corr': np.float32(0.008817076)}\n",
      "Epoch 751/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739833090447, 'test_pearson_corr': np.float32(0.008855268)}\n",
      "Epoch 752/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067896985822606, 'test_pearson_corr': np.float32(0.008891774)}\n",
      "Epoch 753/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057385303374677, 'test_pearson_corr': np.float32(0.008927224)}\n",
      "Epoch 754/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047043122546178, 'test_pearson_corr': np.float32(0.008962307)}\n",
      "Epoch 755/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072699148860245, 'test_pearson_corr': np.float32(0.008996931)}\n",
      "Epoch 756/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011081950254627293, 'test_pearson_corr': np.float32(0.009028667)}\n",
      "Epoch 757/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062466143396225, 'test_pearson_corr': np.float32(0.00906123)}\n",
      "Epoch 758/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047912300418614, 'test_pearson_corr': np.float32(0.009092254)}\n",
      "Epoch 759/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056103134991383, 'test_pearson_corr': np.float32(0.00912309)}\n",
      "Epoch 760/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053706473372834, 'test_pearson_corr': np.float32(0.009152786)}\n",
      "Epoch 761/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046303282139667, 'test_pearson_corr': np.float32(0.009181342)}\n",
      "Epoch 762/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072311690779842, 'test_pearson_corr': np.float32(0.009209092)}\n",
      "Epoch 763/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048831560301292, 'test_pearson_corr': np.float32(0.009237047)}\n",
      "Epoch 764/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051423850624742, 'test_pearson_corr': np.float32(0.009263025)}\n",
      "Epoch 765/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106853986970231, 'test_pearson_corr': np.float32(0.00928903)}\n",
      "Epoch 766/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086184667210316, 'test_pearson_corr': np.float32(0.009313698)}\n",
      "Epoch 767/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065867663990574, 'test_pearson_corr': np.float32(0.009337677)}\n",
      "Epoch 768/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050917523782618, 'test_pearson_corr': np.float32(0.009361183)}\n",
      "Epoch 769/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105334324000234, 'test_pearson_corr': np.float32(0.009384572)}\n",
      "Epoch 770/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067080441099833, 'test_pearson_corr': np.float32(0.009406815)}\n",
      "Epoch 771/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049496234427053, 'test_pearson_corr': np.float32(0.009428559)}\n",
      "Epoch 772/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107206685328804, 'test_pearson_corr': np.float32(0.0094494345)}\n",
      "Epoch 773/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069031753497484, 'test_pearson_corr': np.float32(0.009470305)}\n",
      "Epoch 774/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051947010844814, 'test_pearson_corr': np.float32(0.009490391)}\n",
      "Epoch 775/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105603813997889, 'test_pearson_corr': np.float32(0.009509749)}\n",
      "Epoch 776/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011089566716214368, 'test_pearson_corr': np.float32(0.009528453)}\n",
      "Epoch 777/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086326238477877, 'test_pearson_corr': np.float32(0.009546375)}\n",
      "Epoch 778/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066104647677098, 'test_pearson_corr': np.float32(0.009565188)}\n",
      "Epoch 779/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047110953121882, 'test_pearson_corr': np.float32(0.009581801)}\n",
      "Epoch 780/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055230393959973, 'test_pearson_corr': np.float32(0.009599406)}\n",
      "Epoch 781/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048631854240828, 'test_pearson_corr': np.float32(0.009615746)}\n",
      "Epoch 782/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104606628999525, 'test_pearson_corr': np.float32(0.009631964)}\n",
      "Epoch 783/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106810355188579, 'test_pearson_corr': np.float32(0.009647632)}\n",
      "Epoch 784/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065710918393525, 'test_pearson_corr': np.float32(0.00966263)}\n",
      "Epoch 785/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105515589119017, 'test_pearson_corr': np.float32(0.009677319)}\n",
      "Epoch 786/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065708388398844, 'test_pearson_corr': np.float32(0.009692393)}\n",
      "Epoch 787/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074001005269177, 'test_pearson_corr': np.float32(0.009705784)}\n",
      "Epoch 788/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048451542348397, 'test_pearson_corr': np.float32(0.009720184)}\n",
      "Epoch 789/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279080875696, 'test_pearson_corr': np.float32(0.009733992)}\n",
      "Epoch 790/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060861884526097, 'test_pearson_corr': np.float32(0.009746321)}\n",
      "Epoch 791/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045923327620978, 'test_pearson_corr': np.float32(0.009759426)}\n",
      "Epoch 792/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056462860938256, 'test_pearson_corr': np.float32(0.009770683)}\n",
      "Epoch 793/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061260510713483, 'test_pearson_corr': np.float32(0.00978347)}\n",
      "Epoch 794/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011089148207036044, 'test_pearson_corr': np.float32(0.009794075)}\n",
      "Epoch 795/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105115393775407, 'test_pearson_corr': np.float32(0.009806514)}\n",
      "Epoch 796/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075166604570467, 'test_pearson_corr': np.float32(0.009816988)}\n",
      "Epoch 797/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051051285160892, 'test_pearson_corr': np.float32(0.009828349)}\n",
      "Epoch 798/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066541146362429, 'test_pearson_corr': np.float32(0.0098384265)}\n",
      "Epoch 799/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049737992811664, 'test_pearson_corr': np.float32(0.009848921)}\n",
      "Epoch 800/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045923258548175, 'test_pearson_corr': np.float32(0.009858184)}\n",
      "Epoch 801/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105384947619319, 'test_pearson_corr': np.float32(0.009868483)}\n",
      "Epoch 802/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071906614906035, 'test_pearson_corr': np.float32(0.009878537)}\n",
      "Epoch 803/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104707948823776, 'test_pearson_corr': np.float32(0.009886643)}\n",
      "Epoch 804/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074087464031327, 'test_pearson_corr': np.float32(0.009896657)}\n",
      "Epoch 805/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052549227217876, 'test_pearson_corr': np.float32(0.009904731)}\n",
      "Epoch 806/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106756398898977, 'test_pearson_corr': np.float32(0.009913911)}\n",
      "Epoch 807/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110506146220888, 'test_pearson_corr': np.float32(0.009922395)}\n",
      "Epoch 808/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104592334703076, 'test_pearson_corr': np.float32(0.009931278)}\n",
      "Epoch 809/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075410917347531, 'test_pearson_corr': np.float32(0.009940396)}\n",
      "Epoch 810/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106196665096627, 'test_pearson_corr': np.float32(0.0099510895)}\n",
      "Epoch 811/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051154558650244, 'test_pearson_corr': np.float32(0.009962235)}\n",
      "Epoch 812/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105859817514729, 'test_pearson_corr': np.float32(0.009974813)}\n",
      "Epoch 813/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051930656530665, 'test_pearson_corr': np.float32(0.009993111)}\n",
      "Epoch 814/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011111390480689077, 'test_pearson_corr': np.float32(0.010014217)}\n",
      "Epoch 815/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110851134478143, 'test_pearson_corr': np.float32(0.010048425)}\n",
      "Epoch 816/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105236731916902, 'test_pearson_corr': np.float32(0.010103253)}\n",
      "Epoch 817/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105504447795366, 'test_pearson_corr': np.float32(0.010182472)}\n",
      "Epoch 818/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104987619859642, 'test_pearson_corr': np.float32(0.01031072)}\n",
      "Epoch 819/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106075376141039, 'test_pearson_corr': np.float32(0.010540038)}\n",
      "Epoch 820/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048428997506377, 'test_pearson_corr': np.float32(0.011047557)}\n",
      "Epoch 821/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047755597327904, 'test_pearson_corr': np.float32(0.012336095)}\n",
      "Epoch 822/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109847103184139, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 823/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056809374787075, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 824/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076520488955894, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 825/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062268264024017, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 826/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057622065854741, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 827/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072985306092443, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 828/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001110374241780723, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 829/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062744773102677, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 830/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073548182848239, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 831/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047912353768406, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 832/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056442445750597, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 833/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052867334253459, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 834/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049913494114176, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 835/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071931718152582, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 836/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049191336561794, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 837/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052803991132745, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 838/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011092777844841472, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 839/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075847532057818, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 840/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110719542896695, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 841/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069880717003028, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 842/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055062234109536, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 843/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047110963748469, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 844/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052112693182886, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 845/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077456628409631, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 846/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065710953092577, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 847/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105698385029884, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 848/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066921162797242, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 849/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065708349362409, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 850/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077566642406843, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 851/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048323818605293, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 852/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069243566057439, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 853/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110479527024775, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 854/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070322657041995, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 855/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056545073512265, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 856/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047110903892603, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 857/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106729481932943, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 858/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065471386929055, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 859/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011064932174168162, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 860/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068570624340197, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 861/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052232989593757, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 862/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105678200395743, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 863/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011093088969560173, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 864/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076792586348803, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 865/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071824969553102, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 866/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109534643159141, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 867/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047618724095208, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 868/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052629281944545, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 869/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067023789799867, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 870/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049434782513146, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 871/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067864067048174, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 872/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074966783569391, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 873/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059374332666708, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 874/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066921129941575, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 875/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011089188596733392, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 876/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071695707774486, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 877/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068496293330707, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 878/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106570837321801, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 879/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065710916224835, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 880/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072484910294334, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 881/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046699458357064, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 882/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056175547143258, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 883/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048474169166927, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 884/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065471401459283, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 885/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011087705751366764, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 886/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011076926043242891, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 887/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279090851674, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 888/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070328267119775, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 889/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108656458507813, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 890/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739917669388, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 891/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011063924639136357, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 892/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052201821169665, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 893/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061253386130487, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 894/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105219995436063, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 895/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011095355297416245, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 896/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059412969845026, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 897/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050007948622416, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 898/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110483238101474, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 899/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075081069887465, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 900/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011112628447467214, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 901/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107325459828381, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 902/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067864134169153, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 903/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011082577270901282, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 904/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046305896713288, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 905/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739913982613, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 906/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054758903231811, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 907/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073864074999914, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 908/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068293331631621, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 909/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055229627978385, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 910/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011092010434302633, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 911/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051930738290308, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 912/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048998166935382, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 913/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052484859390762, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 914/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053582725707434, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 915/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011111138263359373, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 916/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078408773452038, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 917/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110501090946261, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 918/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011088111632692331, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 919/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066145002675393, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 920/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070179707137694, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 921/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104660644429325, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 922/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046606445052293, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 923/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105267612165362, 'test_pearson_corr': np.float32(-0.008626187)}\n",
      "Epoch 924/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357811122237, 'test_pearson_corr': np.float32(0.00081079203)}\n",
      "Epoch 925/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072176878454008, 'test_pearson_corr': np.float32(0.0034502163)}\n",
      "Epoch 926/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069513573266161, 'test_pearson_corr': np.float32(0.0034727447)}\n",
      "Epoch 927/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104849191035884, 'test_pearson_corr': np.float32(0.0034956362)}\n",
      "Epoch 928/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086629577054456, 'test_pearson_corr': np.float32(0.0035174035)}\n",
      "Epoch 929/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068496892323104, 'test_pearson_corr': np.float32(0.00353762)}\n",
      "Epoch 930/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046939053581041, 'test_pearson_corr': np.float32(0.003558614)}\n",
      "Epoch 931/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054689673850543, 'test_pearson_corr': np.float32(0.0035775073)}\n",
      "Epoch 932/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069783396136165, 'test_pearson_corr': np.float32(0.0035965105)}\n",
      "Epoch 933/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105603959582102, 'test_pearson_corr': np.float32(0.003614196)}\n",
      "Epoch 934/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054286021071635, 'test_pearson_corr': np.float32(0.003631915)}\n",
      "Epoch 935/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357849508064, 'test_pearson_corr': np.float32(0.003648467)}\n",
      "Epoch 936/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055364544835114, 'test_pearson_corr': np.float32(0.003664907)}\n",
      "Epoch 937/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073151890822754, 'test_pearson_corr': np.float32(0.0036811244)}\n",
      "Epoch 938/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047675356635998, 'test_pearson_corr': np.float32(0.0036948938)}\n",
      "Epoch 939/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109356861955371, 'test_pearson_corr': np.float32(0.0037107402)}\n",
      "Epoch 940/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079054021204706, 'test_pearson_corr': np.float32(0.0037239653)}\n",
      "Epoch 941/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048292267190406, 'test_pearson_corr': np.float32(0.0037368406)}\n",
      "Epoch 942/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056678698805192, 'test_pearson_corr': np.float32(0.0037505454)}\n",
      "Epoch 943/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077728778718598, 'test_pearson_corr': np.float32(0.0037627516)}\n",
      "Epoch 944/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108268783720982, 'test_pearson_corr': np.float32(0.0037748867)}\n",
      "Epoch 945/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055062279868913, 'test_pearson_corr': np.float32(0.0037872368)}\n",
      "Epoch 946/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105253409159952, 'test_pearson_corr': np.float32(0.003797865)}\n",
      "Epoch 947/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059403199242391, 'test_pearson_corr': np.float32(0.0038084402)}\n",
      "Epoch 948/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105105120893141, 'test_pearson_corr': np.float32(0.0038188987)}\n",
      "Epoch 949/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048451703265254, 'test_pearson_corr': np.float32(0.0038288734)}\n",
      "Epoch 950/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048055277865286, 'test_pearson_corr': np.float32(0.0038386674)}\n",
      "Epoch 951/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053714350708406, 'test_pearson_corr': np.float32(0.0038486626)}\n",
      "Epoch 952/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011063013255071163, 'test_pearson_corr': np.float32(0.0038570447)}\n",
      "Epoch 953/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066178282970582, 'test_pearson_corr': np.float32(0.0038652746)}\n",
      "Epoch 954/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054809254159715, 'test_pearson_corr': np.float32(0.0038745622)}\n",
      "Epoch 955/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106517166215038, 'test_pearson_corr': np.float32(0.0038820654)}\n",
      "Epoch 956/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055902511574712, 'test_pearson_corr': np.float32(0.0038895877)}\n",
      "Epoch 957/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073827755501446, 'test_pearson_corr': np.float32(0.0038973512)}\n",
      "Epoch 958/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071190366819196, 'test_pearson_corr': np.float32(0.0039047473)}\n",
      "Epoch 959/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046303286477049, 'test_pearson_corr': np.float32(0.0039116177)}\n",
      "Epoch 960/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011083259416704903, 'test_pearson_corr': np.float32(0.003918221)}\n",
      "Epoch 961/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104952138440889, 'test_pearson_corr': np.float32(0.0039244653)}\n",
      "Epoch 962/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047136177706682, 'test_pearson_corr': np.float32(0.003930663)}\n",
      "Epoch 963/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108996473083095, 'test_pearson_corr': np.float32(0.0039360696)}\n",
      "Epoch 964/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078409105695468, 'test_pearson_corr': np.float32(0.0039426046)}\n",
      "Epoch 965/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074315621146318, 'test_pearson_corr': np.float32(0.0039480277)}\n",
      "Epoch 966/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050504681393337, 'test_pearson_corr': np.float32(0.0039532077)}\n",
      "Epoch 967/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046462541032477, 'test_pearson_corr': np.float32(0.0039573167)}\n",
      "Epoch 968/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108373194038049, 'test_pearson_corr': np.float32(0.003963601)}\n",
      "Epoch 969/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065471403411105, 'test_pearson_corr': np.float32(0.0039679017)}\n",
      "Epoch 970/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011090131452839388, 'test_pearson_corr': np.float32(0.003971792)}\n",
      "Epoch 971/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069869332677573, 'test_pearson_corr': np.float32(0.003976457)}\n",
      "Epoch 972/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070559698957867, 'test_pearson_corr': np.float32(0.003980826)}\n",
      "Epoch 973/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049268140312528, 'test_pearson_corr': np.float32(0.00398464)}\n",
      "Epoch 974/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049165549527039, 'test_pearson_corr': np.float32(0.0039887154)}\n",
      "Epoch 975/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011071708677412911, 'test_pearson_corr': np.float32(0.0039917375)}\n",
      "Epoch 976/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105037829659993, 'test_pearson_corr': np.float32(0.003995668)}\n",
      "Epoch 977/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105886806524671, 'test_pearson_corr': np.float32(0.003999558)}\n",
      "Epoch 978/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104684256104563, 'test_pearson_corr': np.float32(0.004003098)}\n",
      "Epoch 979/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105452304086686, 'test_pearson_corr': np.float32(0.0040044636)}\n",
      "Epoch 980/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048847801192912, 'test_pearson_corr': np.float32(0.004007713)}\n",
      "Epoch 981/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054459004090397, 'test_pearson_corr': np.float32(0.0040109446)}\n",
      "Epoch 982/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516069224905, 'test_pearson_corr': np.float32(0.0040136008)}\n",
      "Epoch 983/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011108046393099292, 'test_pearson_corr': np.float32(0.0040162485)}\n",
      "Epoch 984/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066448199961768, 'test_pearson_corr': np.float32(0.0040186397)}\n",
      "Epoch 985/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047675318250172, 'test_pearson_corr': np.float32(0.004023071)}\n",
      "Epoch 986/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110533102427213, 'test_pearson_corr': np.float32(0.004024988)}\n",
      "Epoch 987/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049434801163886, 'test_pearson_corr': np.float32(0.0040286887)}\n",
      "Epoch 988/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045527031041241, 'test_pearson_corr': np.float32(0.0040327804)}\n",
      "Epoch 989/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068576229646858, 'test_pearson_corr': np.float32(0.0040369784)}\n",
      "Epoch 990/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050751186499925, 'test_pearson_corr': np.float32(0.0040434427)}\n",
      "Epoch 991/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108799957166788, 'test_pearson_corr': np.float32(0.004052995)}\n",
      "Epoch 992/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105077537173966, 'test_pearson_corr': np.float32(0.0040672086)}\n",
      "Epoch 993/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011108830020023542, 'test_pearson_corr': np.float32(0.0040854583)}\n",
      "Epoch 994/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057218636776288, 'test_pearson_corr': np.float32(0.0041184495)}\n",
      "Epoch 995/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052763624098214, 'test_pearson_corr': np.float32(0.0041678026)}\n",
      "Epoch 996/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011091976804956662, 'test_pearson_corr': np.float32(0.0042453017)}\n",
      "Epoch 997/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048458996789263, 'test_pearson_corr': np.float32(0.00437391)}\n",
      "Epoch 998/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051154467998968, 'test_pearson_corr': np.float32(0.0046488997)}\n",
      "Epoch 999/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108299015877885, 'test_pearson_corr': np.float32(0.0051272074)}\n",
      "Epoch 1000/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105059739205731, 'test_pearson_corr': np.float32(0.0065214518)}\n",
      "Epoch 1001/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.010378449340643699, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1002/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325628461496662, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1003/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327583245122485, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1004/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327600833378323, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1005/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329375759322314, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1006/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327165990401665, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1007/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833143793790422, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1008/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833095273966228, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1009/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328376071286094, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1010/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328257617047933, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1011/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328282156218004, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1012/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329350887041332, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1013/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327176727676533, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1014/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832460607750405, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1015/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332194754778894, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1016/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329890437879434, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1017/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332300262106515, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1018/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326905075732715, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1019/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833037594702489, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1020/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329405261844885, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1021/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832746475757323, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1022/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327600600200686, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1023/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326465358233133, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1024/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832464540679835, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1025/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328648356140636, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1026/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325144906601856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1027/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328117888126572, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1028/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283231750108446, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1029/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832704190658919, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1030/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327796280650554, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1031/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328762535455507, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1032/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329798143951087, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1033/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327494215681016, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1034/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833204438851594, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1035/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832917628140812, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1036/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330189160636215, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1037/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833143794900792, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1038/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325600335832504, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1039/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833167980973305, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1040/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328144636932322, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1041/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329467586895394, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1042/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325036878735048, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1043/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328283999431682, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1044/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832819279366563, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1045/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832781293619585, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1046/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329418775043972, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1047/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324800625376838, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1048/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327540695756097, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1049/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325872365302016, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1050/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832618691082683, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1051/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330260546303363, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1052/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328204219369702, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1053/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326022875913032, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1054/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283272264500311, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1055/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328191505636792, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1056/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832848000188876, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1057/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329903729004582, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1058/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832773640951705, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1059/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328473850440693, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1060/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329575459310443, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1061/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832987316052711, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1062/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833109397868641, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1063/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331787493385253, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1064/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832564342928004, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1065/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328448367456387, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1066/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324390665784856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1067/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328459993027005, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1068/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832706886636518, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1069/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328860458958167, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1070/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330359957701404, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1071/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832838228935634, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1072/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283291639452009, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1073/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324373577195378, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1074/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832345406895423, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1075/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329569207929107, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1076/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832843227819963, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1077/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832458138288222, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1078/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833034327994871, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1079/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329268442092104, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1080/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328146902086484, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1081/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329419996450625, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1082/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329025815211892, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1083/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331146854490886, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1084/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331601717432993, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1085/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833109394537532, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1086/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331451351170035, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1087/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832938993874321, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1088/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832864838945173, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1089/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328872728543203, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1090/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833142792236965, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1091/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329376136848005, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1092/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332378198954813, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1093/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324708586833516, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1094/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327465956772494, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1095/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833007395978123, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1096/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330103850933194, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1097/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330952817388157, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1098/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832597930500653, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1099/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328583488343547, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1100/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833014631147001, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1101/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328327314953158, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1102/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832779937858198, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1103/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331074580527988, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1104/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328598833652616, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1105/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326370932394984, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1106/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329146945441, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1107/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324829850306987, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1108/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328283877291017, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1109/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329497400321476, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1110/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832635452113102, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1111/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330375491773318, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1112/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328982455275632, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1113/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330159724735824, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1114/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324416359739403, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1115/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283287370635749, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1116/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328314212590855, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1117/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327253720710602, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1118/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324339411120125, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1119/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330966430520514, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1120/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832423040612799, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1121/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832611123913269, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1122/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332438525339884, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1123/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832755686273873, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1124/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330103850933194, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1125/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832821914273829, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1126/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329572383586413, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1127/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328674860665055, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1128/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327344682195325, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1129/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283285854203868, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1130/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832432489858832, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1131/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832920639463402, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1132/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327356796328605, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1133/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326114903352656, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1134/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832764392682586, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1135/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028323495641195295, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1136/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327917888338626, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1137/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328996001785806, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1138/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331785250438484, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1139/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328864012141165, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1140/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832580931851121, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1141/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832705554192894, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1142/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330938460308108, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1143/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328327314953158, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1144/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028332194710364106, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1145/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327918065997777, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1146/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330319362585663, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1147/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328419497844537, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1148/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327435077391508, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1149/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832865691709092, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1150/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328904351871877, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1151/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283279498669856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1152/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328055663009336, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1153/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330038849891742, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1154/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329844746166833, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1155/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328798777922076, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1156/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832843298883623, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1157/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833048144324881, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1158/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330966397209424, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1159/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283311166968502, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1160/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325323498462247, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1161/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832838147878647, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1162/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328298001193433, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1163/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330752595526275, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1164/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329025815211892, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1165/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330511545371015, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1166/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832891790948575, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1167/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832631423691879, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1168/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832518898827841, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1169/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328559582084196, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1170/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833095326153603, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1171/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328432677932718, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1172/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832742508406433, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1173/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833092294844359, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1174/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832517709621907, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1175/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330966286172456, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1176/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327605841145607, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1177/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832742865945472, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1178/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330254239403545, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1179/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328598866963706, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1180/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832901225759802, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1181/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833089094758922, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1182/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326947569580617, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1183/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325392807738083, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1184/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328635464748574, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1185/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326150435182624, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1186/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326265735970878, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1187/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327311393312123, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1188/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328954551685406, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1189/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832743287885953, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1190/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331601784055173, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1191/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329858325988097, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1192/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327406496475777, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1193/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324878750987985, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1194/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.007558411062598276, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1195/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050506870036087, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1196/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105479963764256, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1197/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107505986898313, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1198/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057558211031433, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1199/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105469162480478, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1200/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072518018503908, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1201/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056744954694511, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1202/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739851307448, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1203/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011107650186737094, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1204/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051091570761081, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1205/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109968193465963, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1206/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069036081336832, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1207/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074364799246242, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1208/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055229598809493, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1209/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052130420061436, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1210/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049198769532622, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1211/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069448016671189, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1212/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068134047798868, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1213/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109694128853634, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1214/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050480945072596, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1215/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001108561184657349, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1216/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066921215279557, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1217/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056705268520457, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1218/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055120713724276, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1219/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107230288285905, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1220/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050681504729757, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1221/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105236728663866, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1222/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106547142835105, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1223/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067897009895073, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1224/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011109369892174873, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1225/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074157764745845, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1226/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052846360085748, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1227/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054556008111532, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1228/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105031292900636, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1229/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104859459266308, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1230/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070718948742175, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1231/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048255908438637, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1232/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107931867131343, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1233/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070325535762154, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1234/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107395380935264, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1235/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011081238062642849, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1236/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367334024552, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1237/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.00110544615900373, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1238/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106951342969883, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1239/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069776330541558, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1240/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080017854623017, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1241/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065067725258514, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1242/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047819242330375, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1243/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105064765927375, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1244/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086326265586513, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1245/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105037831568441, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1246/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069229723737837, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1247/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011083260023721456, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1248/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079882282821667, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1249/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061363812612686, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1250/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057218659439106, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1251/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011096727424828633, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1252/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107847842930635, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1253/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078478430607565, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1254/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047563645204504, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1255/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048055258780806, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1256/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011089970327354413, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1257/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078232825832961, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1258/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048688568107523, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1259/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011077916615317477, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1260/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049505137661806, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1261/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049091637725477, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1262/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105495962554078, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1263/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047755585833843, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1264/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074634695851734, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1265/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011104409854519757, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1266/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049831095574932, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1267/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067728940479043, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1268/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011106169307722408, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1269/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056583935475386, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1270/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011084338554858484, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1271/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068775908273385, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1272/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062069247824095, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1273/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050743751794144, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1274/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011084876586658805, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1275/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069012817573815, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1276/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056308063476148, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1277/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051525641597069, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1278/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051053459056543, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1279/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072720931082146, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1280/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047887186979217, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1281/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052339970676892, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1282/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011069448025454386, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1283/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060729243493488, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1284/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066921177978076, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1285/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056678791516723, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1286/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053043457211187, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1287/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011084337263186246, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1288/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104765024384721, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1289/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068403288158526, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1290/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107972425791392, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1291/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011062480540683802, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1292/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073447794368249, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1293/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070996278535364, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1294/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516053501896, 'test_pearson_corr': np.float32(-0.01667517)}\n",
      "Epoch 1295/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055937204121344, 'test_pearson_corr': np.float32(0.0078523215)}\n",
      "Epoch 1296/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011079142337016784, 'test_pearson_corr': np.float32(0.01031032)}\n",
      "Epoch 1297/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067357781628041, 'test_pearson_corr': np.float32(0.01036195)}\n",
      "Epoch 1298/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060746522754258, 'test_pearson_corr': np.float32(0.010410081)}\n",
      "Epoch 1299/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105004290227942, 'test_pearson_corr': np.float32(0.010458361)}\n",
      "Epoch 1300/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047279146370157, 'test_pearson_corr': np.float32(0.010503865)}\n",
      "Epoch 1301/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011065328428133123, 'test_pearson_corr': np.float32(0.010548306)}\n",
      "Epoch 1302/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051962231584283, 'test_pearson_corr': np.float32(0.01059185)}\n",
      "Epoch 1303/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011088078319975217, 'test_pearson_corr': np.float32(0.010633938)}\n",
      "Epoch 1304/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049704785059183, 'test_pearson_corr': np.float32(0.010674985)}\n",
      "Epoch 1305/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011091177609218069, 'test_pearson_corr': np.float32(0.010714678)}\n",
      "Epoch 1306/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047516053718765, 'test_pearson_corr': np.float32(0.010753938)}\n",
      "Epoch 1307/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053169829317753, 'test_pearson_corr': np.float32(0.010792124)}\n",
      "Epoch 1308/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055633961292463, 'test_pearson_corr': np.float32(0.010828528)}\n",
      "Epoch 1309/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001104977442811822, 'test_pearson_corr': np.float32(0.01086407)}\n",
      "Epoch 1310/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073427530229904, 'test_pearson_corr': np.float32(0.010898241)}\n",
      "Epoch 1311/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055062255579575, 'test_pearson_corr': np.float32(0.010932186)}\n",
      "Epoch 1312/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011075496908323076, 'test_pearson_corr': np.float32(0.01096578)}\n",
      "Epoch 1313/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050439109725964, 'test_pearson_corr': np.float32(0.01099676)}\n",
      "Epoch 1314/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051693697350347, 'test_pearson_corr': np.float32(0.011027657)}\n",
      "Epoch 1315/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011100643193841968, 'test_pearson_corr': np.float32(0.011058279)}\n",
      "Epoch 1316/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056544969306674, 'test_pearson_corr': np.float32(0.011086772)}\n",
      "Epoch 1317/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001105374686415453, 'test_pearson_corr': np.float32(0.011116097)}\n",
      "Epoch 1318/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011078779466359824, 'test_pearson_corr': np.float32(0.01114318)}\n",
      "Epoch 1319/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051456865905915, 'test_pearson_corr': np.float32(0.0111696245)}\n",
      "Epoch 1320/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060143226698483, 'test_pearson_corr': np.float32(0.011196289)}\n",
      "Epoch 1321/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011072902038341927, 'test_pearson_corr': np.float32(0.011220899)}\n",
      "Epoch 1322/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067897059341223, 'test_pearson_corr': np.float32(0.011246034)}\n",
      "Epoch 1323/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050083164349316, 'test_pearson_corr': np.float32(0.0112693515)}\n",
      "Epoch 1324/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011086088709148748, 'test_pearson_corr': np.float32(0.011292435)}\n",
      "Epoch 1325/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011056038155159726, 'test_pearson_corr': np.float32(0.011315969)}\n",
      "Epoch 1326/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011080006752444234, 'test_pearson_corr': np.float32(0.011338143)}\n",
      "Epoch 1327/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046909661097842, 'test_pearson_corr': np.float32(0.011358943)}\n",
      "Epoch 1328/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066384499549245, 'test_pearson_corr': np.float32(0.011379512)}\n",
      "Epoch 1329/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011074573263456045, 'test_pearson_corr': np.float32(0.011400323)}\n",
      "Epoch 1330/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011066010657485557, 'test_pearson_corr': np.float32(0.011418825)}\n",
      "Epoch 1331/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054689668320381, 'test_pearson_corr': np.float32(0.011439111)}\n",
      "Epoch 1332/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053549726257703, 'test_pearson_corr': np.float32(0.01145815)}\n",
      "Epoch 1333/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011045923315151005, 'test_pearson_corr': np.float32(0.011476119)}\n",
      "Epoch 1334/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050068013766993, 'test_pearson_corr': np.float32(0.011492869)}\n",
      "Epoch 1335/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051824849359858, 'test_pearson_corr': np.float32(0.011510656)}\n",
      "Epoch 1336/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001109215368034166, 'test_pearson_corr': np.float32(0.011526997)}\n",
      "Epoch 1337/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067484108951142, 'test_pearson_corr': np.float32(0.011543375)}\n",
      "Epoch 1338/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051692304725553, 'test_pearson_corr': np.float32(0.011559763)}\n",
      "Epoch 1339/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739854126747, 'test_pearson_corr': np.float32(0.011574457)}\n",
      "Epoch 1340/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073994462003746, 'test_pearson_corr': np.float32(0.011589878)}\n",
      "Epoch 1341/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011067627092036412, 'test_pearson_corr': np.float32(0.011604489)}\n",
      "Epoch 1342/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011070086633760189, 'test_pearson_corr': np.float32(0.0116181355)}\n",
      "Epoch 1343/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011060080850163257, 'test_pearson_corr': np.float32(0.011632502)}\n",
      "Epoch 1344/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049674365809325, 'test_pearson_corr': np.float32(0.011645438)}\n",
      "Epoch 1345/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011068959843614395, 'test_pearson_corr': np.float32(0.011658189)}\n",
      "Epoch 1346/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073691198030132, 'test_pearson_corr': np.float32(0.011671553)}\n",
      "Epoch 1347/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011061935572759714, 'test_pearson_corr': np.float32(0.011683631)}\n",
      "Epoch 1348/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011059152515502384, 'test_pearson_corr': np.float32(0.011695857)}\n",
      "Epoch 1349/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011053483184968675, 'test_pearson_corr': np.float32(0.011707743)}\n",
      "Epoch 1350/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046303301224145, 'test_pearson_corr': np.float32(0.011718677)}\n",
      "Epoch 1351/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011049537477612664, 'test_pearson_corr': np.float32(0.011730044)}\n",
      "Epoch 1352/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011047079452020624, 'test_pearson_corr': np.float32(0.011740365)}\n",
      "Epoch 1353/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011055768877281716, 'test_pearson_corr': np.float32(0.011752497)}\n",
      "Epoch 1354/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058161521834307, 'test_pearson_corr': np.float32(0.011761999)}\n",
      "Epoch 1355/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011046739842632686, 'test_pearson_corr': np.float32(0.011771312)}\n",
      "Epoch 1356/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011051591096588877, 'test_pearson_corr': np.float32(0.011783111)}\n",
      "Epoch 1357/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011085788329859194, 'test_pearson_corr': np.float32(0.011792977)}\n",
      "Epoch 1358/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011058525250425345, 'test_pearson_corr': np.float32(0.01180334)}\n",
      "Epoch 1359/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054719199708228, 'test_pearson_corr': np.float32(0.011814236)}\n",
      "Epoch 1360/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054522407934453, 'test_pearson_corr': np.float32(0.011826774)}\n",
      "Epoch 1361/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011097544901112536, 'test_pearson_corr': np.float32(0.0118407365)}\n",
      "Epoch 1362/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011073548193583258, 'test_pearson_corr': np.float32(0.011858572)}\n",
      "Epoch 1363/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050361452812271, 'test_pearson_corr': np.float32(0.01188178)}\n",
      "Epoch 1364/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011050392879310568, 'test_pearson_corr': np.float32(0.011913119)}\n",
      "Epoch 1365/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001107587563633869, 'test_pearson_corr': np.float32(0.011959811)}\n",
      "Epoch 1366/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011048594513939605, 'test_pearson_corr': np.float32(0.012034153)}\n",
      "Epoch 1367/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011054958977535971, 'test_pearson_corr': np.float32(0.01214712)}\n",
      "Epoch 1368/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052371442717697, 'test_pearson_corr': np.float32(0.012316193)}\n",
      "Epoch 1369/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011057122361109954, 'test_pearson_corr': np.float32(0.012637883)}\n",
      "Epoch 1370/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.0011052367362976573, 'test_pearson_corr': np.float32(0.013253041)}\n",
      "Epoch 1371/20972\n",
      "{'test_loss': 0.03678825497627258, 'train_loss': 0.001106877590794808, 'test_pearson_corr': np.float32(0.015194619)}\n",
      "Epoch 1372/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.012691245979954587, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1373/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329497378114084, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1374/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328812357743344, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1375/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330737327943084, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1376/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833037329324134, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1377/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330588960345564, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1378/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327205974814078, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1379/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327430080727918, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1380/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331222259696302, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1381/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331244356053064, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1382/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328955939647515, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1383/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833064716592453, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1384/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330098876477, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1385/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832820594044272, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1386/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328583499447245, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1387/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330192347397216, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1388/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325101413421233, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1389/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283260208661439, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1390/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028325673486987453, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1391/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328648433866514, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1392/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327249379165125, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1393/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327098901865203, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1394/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331252495062867, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1395/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328389295789062, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1396/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328283921705805, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1397/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330966352794636, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1398/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328163868535292, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1399/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328629446544875, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1400/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328133411094792, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1401/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832863494287482, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1402/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330752728770635, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1403/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328163302246752, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1404/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832772086434144, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1405/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833007393757384, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1406/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833049933130446, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1407/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028327963091488568, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1408/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833049682186897, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1409/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832864841165912, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1410/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328419431222357, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1411/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832940641662936, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1412/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324731349412092, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1413/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832989031573877, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1414/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329948632354707, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1415/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283323003065213, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1416/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329448910477263, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1417/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832754344947292, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1418/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328055418728005, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1419/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832642139869724, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1420/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326190897054005, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1421/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283263613054898, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1422/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328510814647562, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1423/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028330144368323058, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1424/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328068998549272, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1425/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328790749949245, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1426/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326112915790916, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1427/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028331043945428333, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1428/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326721531623522, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1429/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028329890293531376, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1430/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833099244648227, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1431/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833289342159308, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1432/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028324324887484622, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1433/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832899599068211, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1434/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02833025417278136, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1435/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.02832679936853855, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1436/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028328598944689583, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1437/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.0283279498669856, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1438/20972\n",
      "{'test_loss': 0.9022083282470703, 'train_loss': 0.028326537221359246, 'test_pearson_corr': np.float32(nan)}\n",
      "Epoch 1439/20972\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_loader, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
